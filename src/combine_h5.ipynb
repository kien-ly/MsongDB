{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from MillionSongDataset import h5py_getter\n",
    "import os\n",
    "import pandas as pd\n",
    "# import boto3\n",
    "# from boto3.session import Session\n",
    "# from pyspark.sql import SparkSession\n",
    "import awswrangler as wr\n",
    "# s3 = boto3.client('s3')\n",
    "\n",
    "# spark = SparkSession.builder.config(\"spark.hadoop.fs.s3a.access.key\", \"xxxxxxxxxxx\").config(\"spark.hadoop.fs.s3a.secret.key\", \"xxxxxxxxxxx\")\\\n",
    "#     .config(\"spark.jars\", \"/home/quangndd/learn_pyspark/course-master-big-data-with-pyspark-and-aws-main/hadoop-aws-3.3.4.jar\")\\\n",
    "#     .appName('sparkdf').getOrCreate()\n",
    "\n",
    "path = '/home/quangndd/learn_pyspark/course-master-big-data-with-pyspark-and-aws-main/MillionSongSubset'\n",
    "\n",
    "data_dict = {\n",
    "    'analysis_sample_rate':[],\n",
    "    'artist_7digitalid':[],\n",
    "    'artist_familiarity':[],\n",
    "    'artist_hotttnesss':[],\n",
    "    'artist_id':[],\n",
    "    'artist_latitude':[],\n",
    "    'artist_location':[],\n",
    "    'artist_longitude':[],\n",
    "    'artist_mbid':[],\n",
    "    'artist_mbtags':[],\n",
    "    'artist_mbtags_count':[],\n",
    "    'artist_name':[],\n",
    "    'artist_playmeid':[],\n",
    "    'artist_terms':[],\n",
    "    'artist_terms_freq':[],\n",
    "    'artist_terms_weight':[],\n",
    "    'audio_md5':[],\n",
    "    'bars_confidence':[],\n",
    "    'bars_start':[],\n",
    "    'beats_confidence':[],\n",
    "    'beats_start':[],\n",
    "    'danceability':[],\n",
    "    'duration':[],\n",
    "    'end_of_fade_in':[],\n",
    "    'energy':[],\n",
    "    'key':[],\n",
    "    'key_confidence':[],\n",
    "    'loudness':[],\n",
    "    'mode':[],\n",
    "    'mode_confidence':[],\n",
    "    'release':[],\n",
    "    'release_7digitalid':[],\n",
    "    'sections_confidence':[],\n",
    "    'sections_start':[],\n",
    "    'segments_confidence':[],\n",
    "    'segments_loudness_max':[],\n",
    "    'segments_loudness_max_time':[],\n",
    "    'segments_loudness_start':[],\n",
    "    'segments_pitches':[],\n",
    "    'segments_start':[],\n",
    "    'segments_timbre':[],\n",
    "    'similar_artists':[],\n",
    "    'song_hotttnesss':[],\n",
    "    'song_id':[],\n",
    "    'start_of_fade_out':[],\n",
    "    'tatums_confidence':[],\n",
    "    'tatums_start':[],\n",
    "    'tempo':[],\n",
    "    'time_signature':[],\n",
    "    'time_signature_confidence':[],\n",
    "    'title':[],\n",
    "    'track_7digitalid':[],\n",
    "    'track_id':[],\n",
    "    'year':[]\n",
    "}\n",
    "\n",
    "def get_data(data, path):\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        data['analysis_sample_rate'].append(h5py_getter.get_analysis_sample_rate(f))\n",
    "        data['artist_7digitalid'].append(h5py_getter.get_artist_7digitalid(f))\n",
    "        data['artist_familiarity'].append(h5py_getter.get_artist_familiarity(f))\n",
    "        data['artist_hotttnesss'].append(h5py_getter.get_artist_hotttnesss(f))\n",
    "        data['artist_id'].append(h5py_getter.get_artist_id(f).decode(\"utf-8\"))\n",
    "        data['artist_latitude'].append(h5py_getter.get_artist_latitude(f))\n",
    "        data['artist_location'].append(h5py_getter.get_artist_location(f).decode(\"utf-8\"))\n",
    "        data['artist_longitude'].append(h5py_getter.get_artist_longitude(f))\n",
    "        data['artist_mbid'].append(h5py_getter.get_artist_mbid(f).decode(\"utf-8\"))\n",
    "        data['artist_mbtags'].append(h5py_getter.get_artist_mbtags(f))\n",
    "        data['artist_mbtags_count'].append(h5py_getter.get_artist_mbtags_count(f))\n",
    "        data['artist_name'].append(h5py_getter.get_artist_name(f).decode(\"utf-8\"))\n",
    "        data['artist_playmeid'].append(h5py_getter.get_artist_playmeid(f))\n",
    "        artist_terms = [item.decode(\"utf-8\") for item in h5py_getter.get_artist_terms(f)]\n",
    "        data['artist_terms'].append(artist_terms)\n",
    "        data['artist_terms_freq'].append(h5py_getter.get_artist_terms_freq(f))\n",
    "        data['artist_terms_weight'].append(h5py_getter.get_artist_terms_weight(f))\n",
    "        data['audio_md5'].append(h5py_getter.get_audio_md5(f).decode(\"utf-8\"))\n",
    "        data['bars_confidence'].append(h5py_getter.get_bars_confidence(f))\n",
    "        data['bars_start'].append(h5py_getter.get_bars_start(f))\n",
    "        data['beats_confidence'].append(h5py_getter.get_beats_confidence(f))\n",
    "        data['beats_start'].append(h5py_getter.get_beats_start(f))\n",
    "        data['danceability'].append(h5py_getter.get_danceability(f))\n",
    "        data['duration'].append(h5py_getter.get_duration(f))\n",
    "        data['end_of_fade_in'].append(h5py_getter.get_end_of_fade_in(f))\n",
    "        data['energy'].append(h5py_getter.get_energy(f))\n",
    "        data['key'].append(h5py_getter.get_key(f))\n",
    "        data['key_confidence'].append(h5py_getter.get_key_confidence(f))\n",
    "        data['loudness'].append(h5py_getter.get_loudness(f))\n",
    "        data['mode'].append(h5py_getter.get_mode(f))\n",
    "        data['mode_confidence'].append(h5py_getter.get_mode_confidence(f))\n",
    "        data['release'].append(h5py_getter.get_release(f).decode(\"utf-8\"))\n",
    "        data['release_7digitalid'].append(h5py_getter.get_release_7digitalid(f))\n",
    "        data['sections_confidence'].append(h5py_getter.get_sections_confidence(f))\n",
    "        data['sections_start'].append(h5py_getter.get_sections_start(f))\n",
    "        data['segments_confidence'].append(h5py_getter.get_segments_confidence(f))\n",
    "        data['segments_loudness_max'].append(h5py_getter.get_segments_loudness_max(f))\n",
    "        data['segments_loudness_max_time'].append(h5py_getter.get_segments_loudness_max_time(f))\n",
    "        data['segments_loudness_start'].append(h5py_getter.get_segments_loudness_start(f))\n",
    "        data['segments_pitches'].append(h5py_getter.get_segments_pitches(f))\n",
    "        data['segments_start'].append(h5py_getter.get_segments_start(f))\n",
    "        data['segments_timbre'].append(h5py_getter.get_segments_timbre(f))\n",
    "        data['similar_artists'].append(h5py_getter.get_similar_artists(f))\n",
    "        data['song_hotttnesss'].append(h5py_getter.get_song_hotttnesss(f))\n",
    "        data['song_id'].append(h5py_getter.get_song_id(f).decode(\"utf-8\"))\n",
    "        data['start_of_fade_out'].append(h5py_getter.get_start_of_fade_out(f))\n",
    "        data['tatums_confidence'].append(h5py_getter.get_tatums_confidence(f))\n",
    "        data['tatums_start'].append(h5py_getter.get_tatums_start(f))\n",
    "        data['tempo'].append(h5py_getter.get_tempo(f))\n",
    "        data['time_signature'].append(h5py_getter.get_time_signature(f))\n",
    "        data['time_signature_confidence'].append(h5py_getter.get_time_signature_confidence(f))\n",
    "        data['title'].append(h5py_getter.get_title(f).decode(\"utf-8\"))\n",
    "        data['track_7digitalid'].append(h5py_getter.get_track_7digitalid(f))\n",
    "        data['track_id'].append(h5py_getter.get_track_id(f).decode(\"utf-8\"))\n",
    "        data['year'].append(h5py_getter.get_year(f))\n",
    "    return data\n",
    "\n",
    "# Loop through every directory to get .h5 file\n",
    "for directories in os.listdir(path): \n",
    "    for middle_dir in os.listdir(os.path.join(path, directories)):\n",
    "        data = data_dict\n",
    "        for sub_dir in os.listdir(os.path.join(path, directories, middle_dir)):\n",
    "            for file in os.listdir(os.path.join(path, directories, middle_dir, sub_dir)):\n",
    "                file_path = os.path.join(path, directories, middle_dir, sub_dir, file)\n",
    "                # print(file_path)\n",
    "\n",
    "                # get all data in .h5 file and add to data dictionary\n",
    "                data = get_data(data, file_path)\n",
    "        # print(data)\n",
    "        # print(os.path.join(path, directories, middle_dir))\n",
    "\n",
    "        # Change data dict into dataframe\n",
    "        df = pd.DataFrame(data)\n",
    "        # print(df.head(2))\n",
    "\n",
    "        # convert df to parquet and upload to s3 bucket\n",
    "        wr.s3.to_parquet(\n",
    "            df=df,\n",
    "            path=f\"s3://1msongdata/parquet/{directories}/{middle_dir}.parquet\"\n",
    "        )\n",
    "        # ddf = spark.createDataFrame(df)\n",
    "        # ddf = ddf.coalesce(1)\n",
    "        # parquet_path = os.path.join(path, directories, middle_dir)\n",
    "        # ddf.write.option(\"compression\",\"snappy\").parquet(f's3a://1msongdata/parquet/{directories}/{middle_dir}.parquet')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
