{
  "metadata": {
    "kernelspec": {
      "name": "glue_pyspark",
      "display_name": "Glue PySpark",
      "language": "python"
    },
    "language_info": {
      "name": "Python_Glue_Session",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "pygments_lexer": "python3",
      "file_extension": ".py"
    },
    "toc-autonumbering": false,
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Glue Studio Notebook\n",
        "You are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n",
        "\n",
        "## Available Magics\n",
        "|          Magic              |   Type       |                                                                        Description                                                                        |\n",
        "|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
        "| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n",
        "| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n",
        "| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n",
        "| %region                     |  String      |  Specify the AWS region in which to initialize a session                                                                                                  |\n",
        "| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n",
        "| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n",
        "| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n",
        "| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n",
        "| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n",
        "| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n",
        "| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X                                                                            |\n",
        "| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0)                                |\n",
        "| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n",
        "| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n",
        "| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n",
        "| %etl                        |  String      |   Changes the session type to Glue ETL.                                                                                                                   |\n",
        "| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n",
        "| %stop_session               |              |  Stops the current session.                                                                                                                               |\n",
        "| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n",
        "| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer                       |"
      ],
      "metadata": {
        "editable": false,
        "deletable": false,
        "tags": [],
        "trusted": true,
        "id": "GgNi_DMr9SJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "from awsglue.transforms import *\n",
        "from awsglue.utils import getResolvedOptions\n",
        "from pyspark.context import SparkContext\n",
        "from awsglue.context import GlueContext\n",
        "from awsglue.job import Job\n",
        "  \n",
        "sc = SparkContext.getOrCreate()\n",
        "glueContext = GlueContext(sc)\n",
        "spark = glueContext.spark_session\n",
        "job = Job(glueContext)"
      ],
      "metadata": {
        "editable": true,
        "tags": [],
        "trusted": true,
        "id": "XTm6HJZt9SJQ",
        "outputId": "663e8662-48bf-492b-ff2c-1d7d33c08549"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3"
      ],
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "trusted": true,
        "id": "bnZfJ78w9SJT",
        "outputId": "cb43eb43-9ea8-49b2-b595-887e98642339"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\nDefaulting to user installation because normal site-packages is not writeable\nCollecting boto3\n  Downloading boto3-1.17.112-py2.py3-none-any.whl (131 kB)\n\u001b[K     |████████████████████████████████| 131 kB 34.5 MB/s eta 0:00:01\n\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\nCollecting s3transfer<0.5.0,>=0.4.0\n  Downloading s3transfer-0.4.2-py2.py3-none-any.whl (79 kB)\n\u001b[K     |████████████████████████████████| 79 kB 13.9 MB/s eta 0:00:01\n\u001b[?25hCollecting botocore<1.21.0,>=1.20.112\n  Downloading botocore-1.20.112-py2.py3-none-any.whl (7.7 MB)\n\u001b[K     |████████████████████████████████| 7.7 MB 76.5 MB/s eta 0:00:01\n\u001b[?25hCollecting futures<4.0.0,>=2.2.0; python_version == \"2.7\"\n  Downloading futures-3.4.0-py2-none-any.whl (16 kB)\nCollecting python-dateutil<3.0.0,>=2.1\n  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\nCollecting urllib3<1.27,>=1.25.4\n  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n\u001b[K     |████████████████████████████████| 140 kB 90.2 MB/s eta 0:00:01\n\u001b[?25hCollecting six>=1.5\n  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\nInstalling collected packages: jmespath, futures, six, python-dateutil, urllib3, botocore, s3transfer, boto3\nSuccessfully installed boto3-1.17.112 botocore-1.20.112 futures-3.4.0 jmespath-0.10.0 python-dateutil-2.8.2 s3transfer-0.4.2 six-1.16.0 urllib3-1.26.13\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, ArrayType\n",
        "from pyspark.sql.functions import col, monotonically_increasing_id\n",
        "import pyspark.sql.functions as f\n",
        "import boto3\n",
        "from pyspark.sql.window import Window\n",
        "w = Window.partitionBy(f.lit(1)).orderBy(f.lit(1))\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "7VuejKdN9SJU",
        "outputId": "278baede-0381-4962-abdc-d8a606eca088"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "schema_fact_song = StructType([\n",
        "                    StructField('song_id', StringType(), True),\n",
        "                    StructField('artist_id', StringType(), True),\n",
        "                    StructField('release', StringType(), True),\n",
        "                    StructField('title', StringType(), True),\n",
        "                    StructField('duration', DoubleType(), True),\n",
        "                    StructField('loudness', DoubleType(), True),\n",
        "                    StructField('artist_hotttnesss', DoubleType(), True),\n",
        "                    StructField('year', IntegerType(), True),\n",
        "                    StructField('start_of_fade_out', DoubleType(), True),\n",
        "                    StructField('end_of_fade_in', DoubleType(), True)\n",
        "])\n",
        "\n",
        "schema_dim_location = StructType([\n",
        "                    StructField('artist_location', StringType(), True)                 \n",
        "])\n",
        "\n",
        "schema_dim_album = StructType([\n",
        "                    StructField('release', StringType(), True)\n",
        "])\n",
        "\n",
        "schema_dim_artist = StructType([\n",
        "                    StructField('artist_id', StringType(), True),\n",
        "                    StructField('artist_location', StringType(), True),\n",
        "                    StructField('artist_familiarity', DoubleType(), True),\n",
        "                    StructField('artist_hotttnesss', DoubleType(), True),\n",
        "                    StructField('artist_name', StringType(), True),\n",
        "                    StructField('artist_longitude', DoubleType(), True),\n",
        "                    StructField('artist_latitude', DoubleType(), True)\n",
        "])\n",
        "\n",
        "schema_dim_term = StructType([\n",
        "                    # StructField('term_id', IntegerType(), True),\n",
        "                    StructField('artist_terms', StringType(), True)\n",
        "])\n",
        "\n",
        "schema_dim_artist_term = StructType([\n",
        "                    StructField('artist_terms', StringType(), True),\n",
        "                    StructField('artist_id', StringType(), True)\n",
        "])\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "dCdXuANG9SJU",
        "outputId": "2b0856da-8066-49df-b7dc-9c319663f58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PARQUET_PATH = 's3://1msongdata/final_parquet/C/A.parquet'"
      ],
      "metadata": {
        "trusted": true,
        "id": "dluHMkCO9SJV",
        "outputId": "33d4ef84-ab0b-43de-93cb-a6a97a0dfce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parquet_to_table(parquet_path):\n",
        "    dataFrame = spark.read.parquet(parquet_path)\n",
        "    numOfDataPoint = dataFrame.count()\n",
        "\n",
        "    # create dim_term table\n",
        "    dim_term_table = dataFrame.filter((f.size(dataFrame.artist_terms) != 0)).select(f.explode(dataFrame.artist_terms).alias('terms'))\n",
        "    dim_term_table = dim_term_table.distinct().withColumn('term_id', f.row_number().over(w))\n",
        "    newRow_term = spark.createDataFrame([('unknown',-1)])\n",
        "    dim_term_table = dim_term_table.union(newRow_term).sort('term_id')\n",
        "    \n",
        "    # create dim_artist_term table\n",
        "    dim_artist_term_table = dataFrame.select('artist_id', 'artist_terms')\n",
        "    dim_artist_term_table = dim_artist_term_table.withColumn('artist_terms', \\\n",
        "        f.when(f.size(f.col('artist_terms')) == 0, f.array(f.lit('unknown')))\\\n",
        "        .otherwise(f.col('artist_terms')))\n",
        "\n",
        "    dim_artist_term_table = dim_artist_term_table.select('artist_id', f.explode(dim_artist_term_table.artist_terms).alias('terms')).distinct()\n",
        "    dim_artist_term_table = dim_artist_term_table.join(dim_term_table, 'terms').select('artist_id', 'term_id')\n",
        "\n",
        "    # create dim_album table\n",
        "    col_dim_album = dataFrame.select(\n",
        "        col('release'),\n",
        "    )\n",
        "    dim_album_table = spark.createDataFrame(col_dim_album.collect(), schema_dim_album)\n",
        "    dim_album_table = dim_album_table.select(\"*\").withColumn(\"album_id\", monotonically_increasing_id())\n",
        "    dim_album_table = dim_album_table.na.fill(value='unknown')\n",
        "\n",
        "    \n",
        "    # create dim_location table\n",
        "    col_dim_location = dataFrame.select(\n",
        "        col('artist_location')\n",
        "    )\n",
        "    dim_location_table = spark.createDataFrame(col_dim_location.collect(), schema_dim_location)\n",
        "    dim_location_table = dim_location_table.select(\"*\").withColumn(\"location_id\", monotonically_increasing_id())\n",
        "    dim_location_table = dim_location_table.na.fill(value='unknown')\n",
        "    \n",
        "    \n",
        "    # create fact_song table\n",
        "    col_fact_song = dataFrame.select(\n",
        "        col(\"song_id\"),\n",
        "        col(\"artist_id\"),\n",
        "        col('release'),\n",
        "        col(\"title\"),\n",
        "        col(\"duration\"),\n",
        "        col(\"loudness\"),\n",
        "        col(\"artist_hotttnesss\"),\n",
        "        col(\"year\"),\n",
        "        col(\"start_of_fade_out\"),\n",
        "        col(\"end_of_fade_in\")\n",
        "    )\n",
        "    fact_song_table = spark.createDataFrame(col_fact_song.collect(), schema_fact_song)\n",
        "    fact_song_table = dim_album_table.join(fact_song_table, \"release\").select(col(\"song_id\"),col(\"artist_id\"),col('release'),col(\"title\"),col(\"duration\"),col(\"loudness\"),col(\"artist_hotttnesss\"),col(\"year\"),col(\"start_of_fade_out\"),col(\"end_of_fade_in\"),col(\"album_id\"))\n",
        "    fact_song_table = fact_song_table.na.fill(value='unknown')\n",
        "\n",
        "    \n",
        "    # create dim_artist table\n",
        "    col_dim_artist = dataFrame.select(\n",
        "        col('artist_id'),\n",
        "        col('artist_location'),\n",
        "        col('artist_familiarity'),\n",
        "        col('artist_hotttnesss'),\n",
        "        col('artist_name'),\n",
        "        col('artist_longitude'),\n",
        "        col('artist_latitude')\n",
        "    )\n",
        "    dim_artist_table = spark.createDataFrame(col_dim_artist.collect(), schema_dim_artist)\n",
        "    dim_artist_table= dim_artist_table.join(dim_location_table, \"artist_location\").select(col('artist_id'), col('artist_location'), col('artist_familiarity'), col('artist_hotttnesss'), col('artist_name'), col('artist_longitude'), col('artist_latitude'), col(\"location_id\"))\n",
        "    dim_artist_table = dim_artist_table.na.fill(value='unknown')\n",
        "    \n",
        "    return dim_term_table, dim_artist_term_table, dim_album_table, dim_location_table, fact_song_table, dim_artist_table\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "HEiodvEu9SJW",
        "outputId": "1906b7f7-3374-4cb6-ca74-fa21d2420268"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply parquet_to_table Function\n",
        "\n",
        "dim_term_table, dim_artist_term_table, dim_album_table, dim_location_table, fact_song_table, dim_artist_table = parquet_to_table(PARQUET_PATH)"
      ],
      "metadata": {
        "trusted": true,
        "id": "C8Kek1hZ9SJY",
        "outputId": "f586b79b-04d6-40b8-ced7-04eac6120fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Write parquet file to S3\n",
        "\n",
        "dim_term_table.write.parquet(\"s3://1msongdata/Clean_parquet/data_for_test/dim_term_table.parquet\")\n",
        "dim_artist_term_table.write.parquet(\"s3://1msongdata/Clean_parquet/data_for_test/dim_artist_term_table.parquet\")\n",
        "dim_album_table.write.parquet(\"s3://1msongdata/Clean_parquet/data_for_test/dim_album_table.parquet\")\n",
        "dim_location_table.write.parquet(\"s3://1msongdata/Clean_parquet/data_for_test/dim_location_table.parquet\")\n",
        "dim_artist_table.write.parquet(\"s3://1msongdata/Clean_parquet/data_for_test/dim_artist_table.parquet\")\n",
        "fact_song_table.write.parquet(\"s3://1msongdata/Clean_parquet/data_for_test/fact_song_table.parquet\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "xFHKOUw69SJZ",
        "outputId": "3a39c57d-a05a-4eef-f39a-6feba5581a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "text": "\n",
          "output_type": "stream"
        }
      ]
    }
  ]
}